---
title: "presentation"
output: html_document
---

```{r}
knitr::opts_chunk$set(fig.width=15, fig.height = 12) 
```

#Load data
```{r, echo=F, warning=F, message=F}
options(width = 100)
source("helper_functions.R")
ps <- loadPhyloseq(dir = "C:/Users/ctata/Documents/Lab/methyl-mercury")

```

#Prune OTUs and clean sample data
```{r}
ps_common <- getCommonOtus(ps, numSamples = 3)

#Samples without hair mercury data:
# "CQR05" "CQR28" "CQR29" "CQR41"  "CQR48"
#Sample without developmental outcome:
#"CQR46"

ps_common <- cleanSampleData(ps_common)
#otu_table(ps_common) <- asinh(otu_table(ps_common))
#otu_table(ps) <- asinh(otu_table(ps))

```


#Cross-validated linear regression with regularization
#How perfectly are we able to explain the development data using subsets of data
### We need to decrease regularization manually for mdi, because the fit is not super good. That said, there does seem to be more information there than random - let's think about how to test if this amount of difference is significant?
```{r}
source("helper_functions.R")
models_pdi <- runLinearRegression(y = "bayley36_pdi_adj", ps = ps_common, 
                    method = "cvglmnet", permutations = 10, reg_strength = 'lambda.min', title = "PDI R^2")
models_mdi <- runLinearRegression(y = "bayley36_mdi_adj", ps = ps_common,
                    method = "cvglmnet", permutations = 10, reg_strength = 0.95, title = "MDI R^2")
```

```{r}
ps_common <- recordSignificance(ps_common, models_pdi[[3]], method = "cvglmnet" ,column_name = "pdi_sig")
ps_common <- recordSignificance(ps_common, models_mdi[[3]], method = "cvglmnet" ,column_name = "mdi_sig")
```

#PERMANOVA
#####Asking the reverse question - how much of the microbiomes structure differences can be explained by each lifestyle characteristic. Maternal hair mercury is a significant (almost) explanatory variable for overall microbiome composition
```{r, echo=F, warning=F, message=F}
perm <- runPermanova(ps_common, method = "bray")
perm
#Having a farmer mother can explain .06 percent of the variance in the microbiome. Being a male can explain .045% of it. 
```



#Constrained correspondence
````{r}
source("helper_functions.R")
dist  <- phyloseq::distance(physeq = ps_common, method = "bray")
plotCCA(ps_common, dist, color = "bayley36_mdi_adj", shape = "Family", type = "sites")
plotCCA(ps_common, dist, color = "Family", shape = "Phylum", type = "species")


plotCCA(ps_common, dist, color = "bayley36_pdi_adj", shape = "Family", type = "sites")
plotCCA(ps_common, dist, color = "Family", shape = "Phylum", type = "species")

#Based on this plot, we should check out the relationship of mdi with Lachnospiraceae
#And bifido for pdi
```

#####Biplot
```{r}

cca <- ordinate(ps_common, "DPCoA")
plot_scree(cca, "Scree Plot for Global Patterns Correspondence Analysis")

plot_ordination(ps_common, cca, type = "sites", color="bayley36_pdi_adj") + geom_point(size=5)
plot_ordination(ps_common, cca, type = "sites", color="bayley36_mdi_adj") + geom_point(size=5)
plot_ordination(ps_common, cca, type = "species", color = "Family", shape = "Phylum") + geom_point(size = 5)
```


####

#Phylogenetic tree
##### Of the taxa that were labeled as "valuable" in the above models (seq7, seq9, seq16, seq64), the first three were associated with lower pdi and the last was associated with higher. We'd like to see where they fall on the phylogenetic tree. Red means negatively associated with pdi, and green means positively associated
```{r, echo=F, warning=F, message=F, out.height = 20, fig.height = 30, fig.width = 40}
#got tree from clustalWal online
colorspalette <- c("gray", "red", "forestgreen")

plot_tree(ps_common, label.tips = "taxa_names", color= "pdi_sig") + scale_color_manual(values=c(colorspalette))
plot_tree(ps_common, label.tips = "taxa_names", color= "mdi_sig")+ scale_color_manual(values=c(colorspalette))
```


#Boxplots
##### lastly, we'd like to directly see how the abunance of the microbial taxa relates to the pdi. 
```{r, echo=F, warning=F, message=F}


otu_dds <- data.frame(t(otu_table(ps)))
colnames(otu_dds) <- paste('seq', seq(1, ncol(otu_dds)), sep ="")
otu_dds <- otu_dds[rownames(df),]
otu_raw <- otu_raw[rownames(df), ]
```


```{r, echo=F, warning=F, message=F}
knitr::opts_chunk$set(fig.width=13, fig.height = 9) 

library(reshape2)
library(ggplot2)
boxplotSigTaxa <- function(ps){
  otu <- otu_table(ps)
  df <- sample_data(ps)
  df_use <- cbind(df, otu)
  df_use$sampleid <- paste("Sample", seq(1, nrow(df_use)), sep = "")
  df_use$pdi_binned <- cut(df_use$bayley36_pdi_adj, breaks = 10)
  df_use$pdi_binned <- factor(df_use$pdi_binned, labels = seq(1, 9))
  df_use$diversity <- vegan::diversity(otu, index = "shannon")
  df_use$seqDepth <- rowSums(otu)
  df_melt <- melt(df_use, "sampleid")
  
  sig_taxa <- rownames(tax_table(ps))[tax_table(ps)[ , 'pdi_sig'] != "0" & !is.na(tax_table(ps)[ , 'pdi_sig'])]
  for(taxa in sig_taxa){
    print(taxa)
    dir = tax_table(ps)[taxa, "pdi_sig"] 
    p <- ggplot(df_use, aes_string("bayley36_pdi_adj", y = taxa))
    p <- p + geom_point(aes(color = diversity, size = seqDepth)) + ggtitle(paste(taxa, ": PDI ", dir ))
    print(p)
  }
  
  sig_taxa <- rownames(tax_table(ps))[tax_table(ps)[ , 'mdi_sig'] != "0" & !is.na(tax_table(ps)[ , 'mdi_sig'])]
  for(taxa in sig_taxa){
    print(taxa)
    dir = tax_table(ps)[taxa, "mdi_sig"] 
    p <- ggplot(df_use, aes_string("bayley36_mdi_adj", y = taxa))
    p <- p + geom_point(aes(color = diversity, size = seqDepth))+ ggtitle(paste(taxa, ": MDI ", dir ))
    print(p)
  }
}

boxplotSigTaxa(ps_common)

```


#Alpha diversity and MDI
```{r}

div_shan = vegan::diversity(otu_raw, index = "shannon")
num_species = apply(otu_dds, 1, function(sample) return(sum(sample > 0)))
div_simp = vegan::diversity(otu_raw, index = "simpson")

df_div = df
df_div$div_shan = div_shan
df_div$div_simp = div_simp
df_div$num_species = num_species

plotCorrelation <- function(div = "div_shan", target = "mdi"){
  if(target == "mdi"){
      mod <- lm(df_div$bayley36_mdi_adj ~ df_div[[div]] )
      plot(df_div[[div]], df_div$bayley36_mdi_adj, ylab = "MDI", xlab = div, main = "MDI as explained by diversity")
      abline(coefficients(mod), lwd = 2, lty =2, col = "red")
  }
  if(target == "pdi"){
      mod <- lm(df_div$bayley36_pdi_adj ~ df_div[[div]] )
      plot(df_div[[div]], df_div$bayley36_pdi_adj, ylab = "PDI", xlab = div, main = "PDI as explained by diversity")
      abline(coefficients(mod), lwd = 2, lty =2, col = "red")
    
  }
}

plotCorrelation(div = "div_shan", target = "mdi")
plotCorrelation(div = "div_simp", target = "mdi")
plotCorrelation(div = "num_species", target = "mdi")

plotCorrelation(div = "div_shan", target = "pdi")
plotCorrelation(div = "div_simp", target = "pdi")
plotCorrelation(div = "num_species", target = "pdi")


cor.test(div_shan, df_div$bayley36_mdi_adj)
cor.test(num_species, df_div$bayley36_mdi_adj)
cor.test(div_simp, df_div$bayley36_mdi_adj)

```


#Correlation between all 14 microbes
```{r}
library(corrplot)
res <- cor(otu_keep)
round(res, 3)
corrplot(res, type = "upper", order = "hclust")
```



#Scale of mdi and pdi
##### The values appear on roughly the same scale, and are lightly correlated
```{r}
tmp = data.frame(mdi = df$bayley36_mdi_adj, pdi = df$bayley36_pdi_adj)
ggplot(melt(tmp)) + geom_boxplot(aes(x = variable, y= value)) + geom_jitter(aes(x = variable, y = value), width = 0.02)

plot(df$bayley36_mdi_adj, df$bayley36_pdi_adj) 
```


#Are maternal and child hair Hg corrrelated?
#####Not really
```{r}
low_mat_thg = df$child_hair_thg[df$maternal_hair_thg_median == 0]
high_mat_thg = df$child_hair_thg[df$maternal_hair_thg_median == 1]
boxplot(low_mat_thg, high_mat_thg, names = c("low_mat_thg", "high_mat_thg"))
```



#Cluster based on co-occurrence:
###PDI
```{r}
knitr::opts_chunk$set(fig.width=15, fig.height = 13) 
library(stats)
set.seed(1)

otu <- t(otu_table(ps_dds))
otu <- otu[ , colSums(otu)> 0]
otu <- otu[rownames(df), ]

clustered = kmeans(t(otu), centers = 15, iter.max = 50)
aggregated = lapply(unique(clustered$cluster), function(cluster) return(rowSums(otu[ , cluster])))
otu_cooccur_clust = matrix(unlist(aggregated), ncol = length(aggregated), byrow = T)
otu_cooccur_clust = otu_cooccur_clust[ , colSums(otu_cooccur_clust) > 0]
colnames(otu_cooccur_clust) <- paste("cluster", 1:ncol(otu_cooccur_clust), sep = "")
 
runLinearTestBasic(y = "bayley36_pdi_adj", df,
                               otu_mat = otu_cooccur_clust, 
                               title =  "PDI variance explained using clustered OTUs by co-abundance")
  

                         
```
###MDI
```{r}
set.seed(1)
clustered = kmeans(t(otu), centers = 13, iter.max = 50)
aggregated = lapply(unique(clustered$cluster), function(cluster) return(rowSums(otu[ , cluster])))
otu_cooccur_clust = matrix(unlist(aggregated), ncol = length(aggregated), byrow = T)
otu_cooccur_clust = otu_cooccur_clust[ , colSums(otu_cooccur_clust) > 0]
colnames(otu_cooccur_clust) <- paste("cluster", 1:ncol(otu_cooccur_clust), sep = "")

runLinearTestBasic(y = "bayley36_mdi_adj", df,
                               otu_mat = otu_cooccur_clust, 
                               title =  "MDI variance explained using clustered OTUs by co-abundance")
```

#Cluster based on co-occurrence using glmnet for feature selection
##IMPORTANT: We are switching models and now areusing mean squared error. A LOW number means good prediction.
###PDI
```{r, warning = F, message=F , results = "hide"}
library(stats)
library(glmnet)
set.seed(1)
otu <- t(otu_table(ps_dds))
otu <- otu[ , colSums(otu)> 0]
otu <- otu[rownames(df), ]

#15 clusters give beautiful results
#sweet spot is between 14-15
clustered = kmeans(t(otu), centers = 15, iter.max = 50)
aggregated = lapply(unique(clustered$cluster), function(cluster) return(rowSums(otu[ , cluster])))
otu_cooccur_clust = matrix(unlist(aggregated), ncol = length(aggregated), byrow = T)
otu_cooccur_clust = otu_cooccur_clust[ , colSums(otu_cooccur_clust) > 0]
colnames(otu_cooccur_clust) <- paste("cluster", 1:ncol(otu_cooccur_clust), sep = "")
 


models = runLinearTestGlmnet(y = "bayley36_pdi_adj", df = df, otu_mat = otu_cooccur_clust, title = "PDI Mean Squared Error using clustered OTUs by coabundance cross validated feature selection")
#basic <- models[[1]]
#basic_merc <- models[[2]] 
#basic_micro <- models[[3]] 
#basic_micro_merc <- models[[4]]

```



###MDI
```{r}
#With this model we can't get p values, but we can see which features were informative. The difference is that in this model, instead of looking at R^2, we're looking at the more generalizable metric mean squared error on the held out set using cross-validation. This mens that the features more likely don't just apply to this dataset. It's good that the same features are selected using both models

set.seed(1)
clustered = kmeans(t(otu), centers = 13, iter.max = 50)
aggregated = lapply(unique(clustered$cluster), function(cluster) return(rowSums(otu[ , cluster])))
otu_cooccur_clust = matrix(unlist(aggregated), ncol = length(aggregated), byrow = T)
otu_cooccur_clust = otu_cooccur_clust[ , colSums(otu_cooccur_clust) > 0]
colnames(otu_cooccur_clust) <- paste("cluster", 1:ncol(otu_cooccur_clust), sep = "")
 

models = runLinearTestGlmnet(y = "bayley36_mdi_adj", df = df, otu_mat = otu_cooccur_clust, title = "MDI Mean Squared Error using clustered OTUs by coabundance cross validated feature selection")
basic <- models[[1]]
basic_merc <- models[[2]] 
basic_micro <- models[[3]] 
basic_micro_merc <- models[[4]]

#With this model we can't get p values, but we can see which features were informative. The difference is that in this model, instead of looking at R^2, we're looking at the more generalizable metric mean squared error on the held out set using cross-validation. This mens that the features more likely don't just apply to this dataset. It's good that the same features are selected using both models



```

#Plot cluster membership on tree
```{r, echo = F, eval = F}
#TBD: Now that some clusters are selected, we need to know what is in them. 
clusters <- lapply(unique(clustered$cluster), function(cluster){
  return(names(clustered$cluster[clustered$cluster == cluster]))
})
names(clusters) <- paste("cluster", unique(clustered$cluster), sep = "")
```

```{r, echo = F, eval = F, fig.height = 20, fig.width = 15}
plotClustersOnTree <- function(model, i=1){
  coefs <- coef(model)[,1]
  coefs <- coefs[coefs != 0]
  cluster_ids <- names(coefs)[grepl("cluster", names(coefs))]
  
  col = ifelse(coefs[cluster_ids][i] > 0, "red", "blue")
  
  #Now put the sequences in the clusters on a phylogenetic tree
  tree <- ape::read.tree("data/tree/phylotree.txt")
  cluster_by_seq <- lapply(clusters[cluster_ids], function(cluster){
    return(dict_esv_seq$seq[match(cluster, dict_esv_seq$esv)] )
  })
  
  labels = rep("", length(tree$tip.label))
  labels[match(cluster_by_seq[[i]] , tree$tip.label)] <- cluster_by_seq[[i]]
  tree$tip.label <- labels

  plot(tree, tip.color = col, cex = 1, main = names(cluster_by_seq)[i])
}

plotClustersOnTree(basic_micro, i = 1) 
plotClustersOnTree(basic_micro, i = 2) 
```


```{r}
library(ape)
tree = read.tree('data/tree.newick')
phy_dists <- cophenetic.phylo(tree)
```

#Cluster by otus
```{r}
library(kmer)
library(ape)



getClusteredOtus <- function(otu_clust_obj, seqIDs_){
  #set otu = your otu table
  otu_clust_agg <-  lapply(unique(otu_clust_obj), function(clusterID){
    seqIDs = names(otu_clust_obj[otu_clust_obj == clusterID])
    representative = seqIDs[grepl("\\*", seqIDs)]
    seqIDs <- gsub("\\*", "", seqIDs)
    tmp = rowSums(otu[ , seqIDs_ %in% seqIDs])
    return(representative = tmp)
  })
  
  repSeqs <-  sapply(unique(otu_clust_obj), function(clusterID){
    seqIDs = names(otu_clust_obj[otu_clust_obj == clusterID])
    representative = seqIDs[grepl("\\*", seqIDs)]
    representative <- gsub("\\*", "", representative)
    return(representative)
  })
  
  names(otu_clust_agg) <- repSeqs
  
  otu_clust_agg_mat <- matrix(unlist(otu_clust_agg), byrow = F, nrow = nrow(otu))
  rownames(otu_clust_agg_mat) <- rownames(otu)
  colnames(otu_clust_agg_mat) <- repSeqs
  
  
  otu_clust_agg_mat <- otu_clust_agg_mat[ , colSums(otu_clust_agg_mat) > 0]
  return(otu_clust_agg_mat)
}

seqs_ = colnames(otu)
seqIDs_ = paste("SEQ", seq(1, ncol(otu)), sep = "")
bin = as.DNAbin(strsplit(colnames(otu), ""))
otu_99 = otu(bin, threshold = 0.99) #Gives cluster membership
otu_98 = otu(bin, threshold = 0.98) #Gives cluster membership
otu_97 = otu(bin, threshold = 0.97) #Gives cluster membership
otu_95 = otu(bin, threshold = 0.95) #Gives cluster membership
otu_90 = otu(bin, threshold = 0.90) #Gives cluster membership
otu_80 = otu(bin, threshold = 0.80) #Gives cluster membership
otu_75 = otu(bin, threshold = 0.75) #Gives cluster membership
otu_73 = otu(bin, threshold = 0.73) #Gives cluster membership
otu_70 = otu(bin, threshold = 0.70) #Gives cluster membership

otu_99_agg_mat <- getClusteredOtus(otu_99, seqIDs_)
otu_98_agg_mat <- getClusteredOtus(otu_98, seqIDs_)
otu_97_agg_mat <- getClusteredOtus(otu_97, seqIDs_)
otu_95_agg_mat <- getClusteredOtus(otu_95, seqIDs_)
otu_90_agg_mat <- getClusteredOtus(otu_90, seqIDs_)
otu_80_agg_mat <- getClusteredOtus(otu_80, seqIDs_)
otu_75_agg_mat <- getClusteredOtus(otu_75, seqIDs_)
otu_73_agg_mat <- getClusteredOtus(otu_73, seqIDs_)
otu_70_agg_mat <- getClusteredOtus(otu_70, seqIDs_)

```

##Finding the right level of clustering. We're looking for the point where our information is giving us lower scores than the random columns.

###Run cvglmnet using 99% clustered otus
```{r, warning=F, results = "hide"}

runLinearTestGlmnet(y = "bayley36_pdi_adj", df = df, otu_mat = otu_99_agg_mat, title = "PDI Mean Squared Error using clustered OTUs at 99% and cross validated feature selection")

```

###Phy cluster 98
```{r, include = F, eval = F, fig.width = 15, warning = F, message=F , results = "hide"}
runLinearTestGlmnet(y = "bayley36_pdi_adj", df = df, otu_mat = otu_98_agg_mat, title = "PDI Mean Squared Error using clustered OTUs at 98% and cross validated feature selection")
```

###Phy cluster 97
```{r,  fig.width = 15, warning = F, message=F , results = "hide"}
runLinearTestGlmnet(y = "bayley36_pdi_adj", df = df, otu_mat = otu_97_agg_mat, title = "PDI Mean Squared Error using clustered OTUs at 97% and cross validated feature selection")
```

###Phy cluster 95
```{r,  fig.width = 15, warning = F, message=F , results = "hide"}
runLinearTestGlmnet(y = "bayley36_pdi_adj", df = df, otu_mat = otu_95_agg_mat, title = "PDI Mean Squared Error using clustered OTUs at 95% and cross validated feature selection")
```

###Phy cluster 90
```{r, include = F, eval = F, fig.width = 15, warning = F, message=F , results = "hide"}
runLinearTestGlmnet(y = "bayley36_pdi_adj", df = df, otu_mat = otu_90_agg_mat, title = "PDI Mean Squared Error using clustered OTUs at 90% and cross validated feature selection")
```

###Phy cluster 80
```{r,  fig.width = 15, warning = F, message=F , results = "hide"}
runLinearTestGlmnet(y = "bayley36_pdi_adj", df = df, otu_mat = otu_80_agg_mat, title = "PDI Mean Squared Error using clustered OTUs at 80% and cross validated feature selection")
```

###Phy cluster 75
```{r,  fig.width = 15, warning = F, message=F , results = "hide"}
runLinearTestGlmnet(y = "bayley36_pdi_adj", df = df, otu_mat = otu_75_agg_mat, title = "PDI Mean Squared Error using clustered OTUs at 75% and cross validated feature selection")
```

###Phy cluster 73
```{r,  fig.width = 15, warning = F, message=F , results = "hide" }

runLinearTestGlmnet(y = "bayley36_pdi_adj", df = df,  otu_mat = otu_73_agg_mat, title = "PDI Mean Squared Error using clustered OTUs at 73% and cross validated feature selection")
```











